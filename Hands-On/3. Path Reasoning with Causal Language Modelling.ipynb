{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1YH9sXAybc0MYmuT3542QwuLUgxXUgF0G","timestamp":1709564964863}],"collapsed_sections":["f0kmPStJ60qz","Z-fecI50DkgS","9VH5X8UnPKxo","mXlKyobmESdU","OWf1hNRpEbVN","YitklJg1bqj-"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Hands-on Part 3/4: Language Models for Path Reasoning**\n","\n","---"],"metadata":{"id":"hqaGF-ZlerDz"}},{"cell_type":"markdown","source":["![](https://drive.google.com/uc?id=1k0--CkCfmWKYyYZ9_z4bJIq0ugb-fWDB)"],"metadata":{"id":"EoeJ975sxnMd"}},{"cell_type":"markdown","source":["## **Acknowledgment**\n","\n","---\n","\n","The code use in this tutorial directly derive from our [PEARLM Library](https://github.com/Chris1nexus/pearlm). If this tutorial is useful for your research, we would appreciate an acknowledgment by citing our paper:\n","\n","> Balloccu, G., Boratto, L., Cancedda, C., Fenu, G., & Marras, M. (2023). Faithful Path Language Modelling for Explainable Recommendation over Knowledge Graph. ArXiv, abs/2310.16452.\n","\n"],"metadata":{"id":"ABsCf7n9erXQ"}},{"cell_type":"markdown","source":["## **Get Started**\n","---\n"],"metadata":{"id":"Dl8_7PEz4soy"}},{"cell_type":"markdown","source":["### This notebook"],"metadata":{"id":"bEINDXgYV_8t"}},{"cell_type":"markdown","source":["By now you should already have the Tutorial folder in your google drive. You just need to mount your drive executing the following line.\n"],"metadata":{"id":"MxJi5fuQ5jQX"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"XAiRFZqI6wGA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711212671794,"user_tz":-60,"elapsed":20177,"user":{"displayName":"Giacomo Balloccu","userId":"17057429141399105680"}},"outputId":"d6ceb456-f232-4934-e158-aaa3053c342a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["And browse the working directory."],"metadata":{"id":"OT5H8wSj6w8r"}},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":252,"status":"ok","timestamp":1711216136282,"user":{"displayName":"Giacomo Balloccu","userId":"17057429141399105680"},"user_tz":-60},"id":"K6B1UsCj6zY6","outputId":"3bf5cc72-fc77-4c1d-8bda-350879916ebf"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/ExpRecSys Tutorial Series/2024 ECIR/Hands-On\n"]}],"source":["# Your path to hands on\n","%cd '/content/drive/MyDrive/ExpRecSys Tutorial Series/2024 ECIR/Hands-On'"]},{"cell_type":"markdown","metadata":{"id":"GSUA_rA57Dqt"},"source":["If you followed Part1, you are ready! ü§ò You can skip the next lines."]},{"cell_type":"markdown","metadata":{"id":"f0kmPStJ60qz"},"source":["### Instead, if you joined late\n","\n","Open the google drive folder [https://tinyurl.com/ecir2024-tutorial1](https://tinyurl.com/ecir2024-tutorial1) containing the material and follow the instrucions inside `GetStarted.ipynb`"]},{"cell_type":"markdown","source":["# Outline\n","\n","---\n","\n","\n","\n"],"metadata":{"id":"2xsAm3VZffk1"}},{"cell_type":"markdown","source":["- [ 0 - Packages](#0)\n","- [ 1 - Prerequisites](#1)\n","- [ 2 - Path Sampling](#2)\n","    - [ 2 - Tokenized datasets creation](#2.1)\n","- [ 3 - PLM pipeline](#3)\n","    - [ 3 - Train](#3.1)\n","    - [ 3 - Evaluate](#3.2)\n","    - [ 3 - Textual explanations](#3.3)\n","- [ 4 - PEARLM pipeline](#4)\n","    - [ 3 - Train](#4.1)\n","    - [ 3 - Evaluate](#4.2)\n","    - [ 3 - Textual explanations](#4.3)"],"metadata":{"id":"jF-65kLt6Hc3"}},{"cell_type":"markdown","source":["In the **previous part** we:\n","\n","1Ô∏è‚É£ Mapped our dataset in standard format to a **PGPR readable format** and **CAFE readable format**.\n","\n","2Ô∏è‚É£ Trained the **TransE embedding** [[11]](#p11) used by both PGPR [[10]](#p10) and CAFE [[12]](#p12).\n","\n","3Ô∏è‚É£ **Trained and extracted the predicted paths** from the models.\n","\n","4Ô∏è‚É£ **Evaluated the models** and converted their path into **texual explanations via templates** [[33]](#p33)."],"metadata":{"id":"yG6BGvVHe05O"}},{"cell_type":"markdown","source":["In **this part**, you will learn about how the **causal language modelling** is used for **path reasoning** by PLM [[37]](#p37) and PEARLM [[38]](#p38) pipelines. This will include sample paths from the KG, converting them to tokenized sequences, **train** the models using them and **produce** **recommendations** and **explanation paths**.\n","\n","In this part, we will:\n","\n","1Ô∏è‚É£ Sample paths from an existing knowledge graph which will be used as **training data for PLM and PEARLM**.\n","\n","2Ô∏è‚É£ Train the **PLM** [[37]](#p37) and **PEARLM [[38]](#p38)** models and use their decoding to generate paths.\n","\n","3Ô∏è‚É£ **Evaluate the models** and convert their path into **texual explanations via templates** [[33]](#p33).\n","\n","4Ô∏è‚É£ **Measure the hallucination phenomena** [[38]](#p38) in PLM and see how PEARLM's constraint decoding solves it."],"metadata":{"id":"6xGnG4zQe2T-"}},{"cell_type":"markdown","source":["<a name=\"0\"></a>\n","\n","## 0 - Packages\n","\n","---"],"metadata":{"id":"BRnFs68HDhk3"}},{"cell_type":"code","source":["!pip install . #Takes around 1 min\n","!pip install datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZISrbXPaw01I","executionInfo":{"status":"ok","timestamp":1711105720578,"user_tz":-60,"elapsed":22096,"user":{"displayName":"Giacomo Balloccu","userId":"17057429141399105680"}},"outputId":"12884ad7-4b9d-4ff4-f931-0ba5498a5f2f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing /content/drive/MyDrive/ExpRecSys Tutorial Series/2024 ECIR/Hands-On\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: pathlm\n","  Building wheel for pathlm (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathlm: filename=pathlm-0.0.0-py3-none-any.whl size=201903 sha256=b316365e4d031dea72319f2ae04f0c160d58519fbc53424d01a12ea29ca9f130\n","  Stored in directory: /root/.cache/pip/wheels/5d/d6/fa/857159ee5e51c820ba3cb52d5f60b61adc5ab379954711150d\n","Successfully built pathlm\n","Installing collected packages: pathlm\n","  Attempting uninstall: pathlm\n","    Found existing installation: pathlm 0.0.0\n","    Uninstalling pathlm-0.0.0:\n","      Successfully uninstalled pathlm-0.0.0\n","Successfully installed pathlm-0.0.0\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n","Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"]}]},{"cell_type":"code","source":["import gzip\n","import pickle\n","import random"],"metadata":{"id":"xS1KUfEDjlMJ","executionInfo":{"status":"ok","timestamp":1711216241358,"user_tz":-60,"elapsed":261,"user":{"displayName":"Giacomo Balloccu","userId":"17057429141399105680"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"1\"></a>\n","\n","## 1 - Prerequisites\n","\n","---\n"],"metadata":{"id":"Z-fecI50DkgS"}},{"cell_type":"markdown","source":["üîç Before diving into the core concepts, it's essential to get familiar with some foundational blocks. Here's a brief overview of what you need to know:\n"],"metadata":{"id":"RKOCPerMLQZ6"}},{"cell_type":"markdown","source":["### 1. - Transformer Architecture Fundamentals üõ†\n","\n","The transformer architecture represents a paradigm shift in natural language processing (NLP), setting new standards for a range of tasks from translation to text generation. Central to the transformer's success is its novel attention mechanism. Unlike previous models that processed inputs sequentially, transformers employ attention to simultaneously assess the relevance of all parts of the input data. This enables the model to dynamically prioritize which parts of the input to focus on, dramatically improving its ability to understand the nuanced interplay of context and meaning in language.\n","\n","![](https://drive.google.com/uc?id=1wKt8Kevq_XRX9fVzNxBRcZYCFdQMT3Qy)\n","\n","*Image source: [https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)*\n","\n","**Key Concepts:**\n","- **Attention Mechanism:** The core of the transformer model, enabling dynamic focus on different segments of input data, enhancing the model's interpretability and performance on complex tasks.\n","- **Self-Attention:** Allows each input component to be contextualized in relation to the whole input, significantly enriching the model's understanding of internal relationships within the data.\n","- **Positional Encoding:** Injects information about the sequence order of the input data, compensating for the transformer's inherently order-agnostic design and ensuring sensitivity to the sequence dynamics of language.\n","\n","### 2. Causal Language Modeling (CLM) üìö\n","\n","Causal Language Modeling (CLM) is implemented as a decoder only architecture and lies at the heart of generative language tasks, teaching models to predict subsequent tokens based on preceding context in a manner analogous to human language processing.\n","\n","![](https://drive.google.com/uc?id=1RAC0LkRRq-4yCS2dheDBqX86Ogm1OseN)\n","\n","*Image source: [https://en.rattibha.com/thread/1640446114519474176](https://en.rattibha.com/thread/1640446114519474176)*\n","\n","Unlike models that treat language as a bag of words or follow strict sequential processing, CLM approaches language with an inherently sequential perspective. This paradigm leverages the temporal nature of language, where the meaning and likelihood of a word are heavily influenced by the words that come before it.\n","\n","**Foundational Principles:**\n","- **Sequential Prediction:** The model is trained to anticipate the next word in a sequence, given all the previous words, mirroring the forward-looking nature of human language comprehension and generation.\n","- **Contextual Awareness:** Through the sequential prediction task, the model develops a nuanced understanding of how context shapes meaning, enabling more accurate and coherent text generation.\n","- **Supervised Learning Framework:** The training process involves presenting the model with sequences of tokens, where the target for each input sequence is the same sequence shifted by one position to the right. This framework not only teaches the model about language structure but also about the probabilistic nature of language, where multiple continuations can be valid for any given context.\n","\n","Incorporating these preliminary concepts is crucial for delving into the advanced topic of path language modeling for explainable recommendations. By understanding transformers and CLM, participants will be better equipped to grasp how these models can be adapted and extended to provide not just effective, but also interpretable, recommendations.\n","\n","![](https://drive.google.com/uc?id=16Gn9XYKEOv1duRnRbNbxZRdMalpGkYWV)\n","\n","\n"],"metadata":{"id":"biBftC2RdhqP"}},{"cell_type":"markdown","source":["### 3. Tokenizers: The Gateway to Language Understanding üóù\n","\n","Tokenizers are foundational to the field of natural language processing (NLP), serving as the bridge between the nuanced, variable world of human language and the structured, numerical realm of machine learning models. By decomposing text into manageable units called tokens and translating these tokens into numerical identifiers, tokenizers effectively prepare raw text for deep learning algorithms. The design and choice of tokenizer can significantly impact the performance and capabilities of an NLP model, making an understanding of different tokenization strategies crucial.\n","\n","**Key Points:**\n","\n","- **Tokenization:** This critical preprocessing step involves breaking down complex text into simpler units (tokens) that could be words, characters, or subwords. The nature of these tokens fundamentally influences how a model perceives and processes language data.\n","\n","- **Vocabulary Management:** The tokenizer constructs a vocabulary, an exhaustive list of unique tokens it recognizes. This vocabulary is the model's linguistic repertoire, determining which tokens can be directly processed and how unseen tokens are handled.\n","\n","\n","Before the advent of sophisticated tokenization methods, **character-level** and **word-level tokenization** were common. Character-level tokenization offers granularity and a solution to the out-of-vocabulary issue but at the cost of increased sequence length and complexity. Word-level tokenization, on the other hand, is intuitive and aligns closely with human language processing but struggles with languages rich in morphology and unseen words.\n","\n","**Word-Level Tokenizer:** This approach is straightforward, mapping entire words to numerical identifiers based on a predetermined vocabulary. While intuitive, its main challenge lies in handling out-of-vocabulary (OOV) words, not present in the tokenizer's initial vocabulary list, which can limit its effectiveness.\n","\n","![](https://drive.google.com/uc?id=1xWUhD13bTTLU6l8ic7FOQlnJQ5pgDcdr)\n","\n","*Image source: [https://towardsdatascience.com/byte-pair-encoding-for-beginners-708d4472c0c7](https://towardsdatascience.com/byte-pair-encoding-for-beginners-708d4472c0c7)*\n","\n","To address the limitations of these earlier methods, modern tokenizers frequently utilize **subword tokenization**, a technique that enhances a model's ability to manage diverse linguistic phenomena by splitting unknown or rare words into smaller, recognizable units within the model's vocabulary. The most prominent example of subword tokenization is the Byte Pair Encoding (BPE).\n","\n","**Byte Pair Encoding (BPE):** Originally a data compression algorithm, BPE iteratively merges the most frequent pairs of bytes or characters in a text corpus until it achieves a predefined vocabulary size. Adapted for NLP, BPE merges characters or sequences of characters to form frequently occurring subwords. This approach allows the model to efficiently process common word parts and decode rare or novel words from these components.\n","\n","![](https://drive.google.com/uc?id=1dHmWdKD71XOgMX0dfHcj2ZYu2rpk7a5h)\n","\n","*Image source: [https://medium.com/illuminations-mirror/on-tokenization-in-llms-34309273f238](https://medium.com/illuminations-mirror/on-tokenization-in-llms-34309273f238)*\n","\n","\n","#### The Special Case of Path Language Modeling\n","\n","In the context of path language modeling for explainable recommendations, the choice of tokenizer is crucial, given the specific challenges:\n","\n","1. **No Benefit in Splitting:** For path tokens representing entities and relations (e.g., E202 R1 P2001), splitting into subwords offers no benefit. These tokens derive their meaning from their entirety, not from constituent parts. Splitting them could lead to a loss of semantic information, detrimental to the model's understanding.\n","   \n","2. **Loss of Boundary Information:** BPE and similar subword tokenization methods risk losing essential boundary information between entities and relations. This boundary clarity is crucial for accurately interpreting and navigating paths within the knowledge graph.\n","\n","#### Embracing Word-Level Tokenization\n","\n","Considering these unique requirements, **Word-Level Tokenization** stands out as the preferred method for path language modeling:\n","\n","- **Integrity Preservation:** By treating each entity, relation, and path identifier as an indivisible token, it ensures the full meaning and specificity encoded within each token are retained. This is crucial for models that depend on precise token interpretations to generate meaningful recommendations and explanations.\n","\n","- **Simplified Vocabulary Management:** While potentially larger, the vocabulary under this approach facilitates straightforward and unambiguous token recognition, avoiding the complexities tied to subtoken recombination.\n","\n","- **Optimized Model Performance:** In path language modeling, where the accurate representation of user-centric paths is key, Word-Level Tokenization directly supports improved model performance, enabling the generation of coherent and relevant paths for effective recommendations and explanations.\n","\n","With this foundational knowledge, you're well-equipped to delve into the complexities of path reasoning through causal language modeling. Let's embark on this journey together! üöÄ\n"],"metadata":{"id":"rx6mnpWsURlh"}},{"cell_type":"markdown","source":["### 4. - Causal Language Modeling for Path Reasoning\n","\n","Causal Language Modeling (CLM) intricately mirrors human language generation, where sentences are crafted word by word. In human language, CLMs consider words as the building blocks of communication, weaving them into coherent and meaningful sentences. Similarly, in path reasoning, entities (E), products (P), users (U), and relationships (R) within a knowledge graph act as the 'words' of our narrative. These elements form paths that narrate stories about user preferences, item relationships, and the intricate web of connections that define the recommendation landscape.\n","\n","#### Path Sampling: Crafting User-Centric Narratives\n","\n","The initial step in preparing our data for causal language modeling and path reasoning involves a meticulous path sampling process from the knowledge graph. This stage is crucial for extracting meaningful, user-centric paths that highlight the user's interactions and preferences.\n","\n","**Focusing on User-Centric Paths**\n","\n","Path sampling is deliberately designed to capture sequences that begin with a user and unfold through their interactions. For example, a path might start with \"U20 R-1 P20,\" indicating that user U20 has interacted with product P20. The objective is to extend these paths to include other related products or entities, revealing the intricate patterns and connections that map out the user‚Äôs engagement within the graph.\n","\n","**Illustrative Example**\n","\n","Consider a path like \"U20 R-1 P20 R2 E5 R2 P45,\" where the user's interaction with P20 leads us through a related entity E5 to another product P45 (also interated). This path not only traces the user's direct interactions but also the relational context that influences these interactions, enriching our understanding of their preferences.\n","\n","\n","![](https://drive.google.com/uc?id=1KIOKHz85eDgxle-HTsoe92Q8OXYvFBTd)\n","\n","\n","#### Path Tokenization and Training for Reasoning\n","Once paths are sampled, we tokenize them using specific prefixes (E for entities, P for products, U for users, and R for relationships), creating a vocabulary that mirrors the knowledge graph‚Äôs structure. For example, \"U20\" denotes user 20. This tokenization allows our model to differentiate between node and edge types, ensuring accurate path interpretation. During training, the model learns to reconstruct and reason with these tokenized paths, predicting subsequent nodes or edges by analyzing the context of preceding elements. This process equips the model with the essential ability to explore the knowledge graph, unveiling patterns and connections critical for generating personalized recommendations.\n","\n","#### Leveraging Path Reasoning for Recommendations and Explanations\n","\n","**Path Prediction:** Upon receiving a prompt that begins with a user and their interaction‚Äîsay, \"U2 R-1\"‚Äîour model is tasked with generating a set of potential paths forward. This is not a mere extrapolation of the most likely next step but a comprehensive prediction of a series of connected actions and relationships, encapsulating a wide range of possible user journeys.\n","\n","**Ranking and Selection:** The generated paths are then ranked based on their cumulative probability, a measure that reflects the model's confidence in each path's relevance and accuracy in representing potential user behavior. This ranking process is pivotal, as it sifts through the multitude of possibilities to highlight the paths most significant and likely to resonate with the user's preferences and past interactions.\n","\n","**Recommendations and Explanations**: From the ranked list, we select the top 10 unique paths (in terms of item reached), ensuring diversity in the recommendations by focusing on the uniqueness of the terminal item in each sequence.\n","\n","These paths like the previous seen path reasoning models serve a dual purpose:\n","\n","- **Explanations**: The sequence of interactions leading up to the final product in each path provides a narrative explanation for the recommendation.\n","- **Recommendations**: The last item of each unique path represents a recommended product.\n"],"metadata":{"id":"9VH5X8UnPKxo"}},{"cell_type":"markdown","source":["<a name=\"2\"></a>\n","\n","## 2 - Path Sampling\n","\n","---"],"metadata":{"id":"O6WAO0halIxS"}},{"cell_type":"markdown","source":["In this part of the tutorial, you can chose to proceed with `ml1m` or `lfm1m`. We will use `ml1m` but you are free to chose. All the pre-trained models for these datasets are available, while for the `cellphones` dataset we will release it soon, in the [offical github tutorial repository](https://github.com/explainablerecsys/ecir24) to reduce the size of this tutorial folder."],"metadata":{"id":"rqF4VOqlw9id"}},{"cell_type":"code","source":["dataset_name = \"ml1m\""],"metadata":{"id":"QZ0qVqdpEANT","executionInfo":{"status":"ok","timestamp":1711216251582,"user_tz":-60,"elapsed":248,"user":{"displayName":"Giacomo Balloccu","userId":"17057429141399105680"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["To use **Causal Language Models (CLM) for path reasoning** the first step is to sample **user-centric paths** from the knowledge graph. This paths will be tokenized and used as training sequences by our CLM based path reasoning models.\n","\n","The sampled paths will start from a user, connect him to a seen product through its interaction in the train and bring to another seen products. This will allow the data to have patterns between the items interacted by each user.\n","\n","To perform the sampling we can employ our `create_dataset.sh` script giving as positional parameters:\n","1. `dataset`: the dataset we want to sample for `{ml1m, lfm1m}`\n","2. `sample_size`: represent the amount of paths sampled for each user\n","3. `n_hop`: represent the fixed hop size for the paths sampled\n","4. `n_proc`: number of processors to employ for multiprocessing operations\n"],"metadata":{"id":"Fgp8Q6ibnR2_"}},{"cell_type":"code","source":["sample_size = 250\n","n_hop = 3\n","n_proc = 2"],"metadata":{"id":"V4uD8BoM9I1W","executionInfo":{"status":"ok","timestamp":1711216141848,"user_tz":-60,"elapsed":261,"user":{"displayName":"Giacomo Balloccu","userId":"17057429141399105680"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["‚ö†Ô∏è We already have the **sampled paths for all datasets**. So you **don't need** to run this command now. The `paths_end-to-end_250_3.txt` file will be store in `data/<dataset>/path_random_walk/`\n","\n","‚è≤Ô∏è Estimate time: 20m with `ML1M`"],"metadata":{"id":"f0bTWqzsAweN"}},{"cell_type":"code","source":["! bash create_dataset.sh {dataset_name} {sample_size} {n_hop} {n_proc}"],"metadata":{"id":"gVP2x-JM7Tw4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code will create the dataset into `data/<dataset>/path_random_walk/paths_end-to-end_<sample_size>_<n_hop>.txt`"],"metadata":{"id":"E2-C--uU9gF5"}},{"cell_type":"code","source":["! ls data/{dataset_name}/paths_random_walk"],"metadata":{"id":"_lLzollZ9fAi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711216287565,"user_tz":-60,"elapsed":337,"user":{"displayName":"Giacomo Balloccu","userId":"17057429141399105680"}},"outputId":"c04b3b00-1092-4cb9-8aa4-163e0db805a9"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["paths_end-to-end_250_3.txt\n"]}]},{"cell_type":"markdown","source":["Let's see how the paths look like"],"metadata":{"id":"gcyp5UaR9-fx"}},{"cell_type":"code","source":["! head -10 data/{dataset_name}/paths_random_walk/paths_end-to-end_250_3.txt"],"metadata":{"id":"VaqDtFs399gG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711216291554,"user_tz":-60,"elapsed":371,"user":{"displayName":"Giacomo Balloccu","userId":"17057429141399105680"}},"outputId":"db220d2c-c5a7-4682-f549-029172c92851"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["U5683 R-1 P2224 R-1 U423 R-1 P2386\n","U5992 R-1 P2133 R8 E3960 R8 P867\n","U2255 R-1 P601 R3 E3912 R3 P2712\n","U2286 R-1 P2619 R8 E4994 R8 P810\n","U1895 R-1 P226 R9 E3495 R9 P1297\n","U4887 R-1 P1625 R-1 U2409 R-1 P722\n","U4423 R-1 P226 R-1 U3617 R-1 P2484\n","U5906 R-1 P1504 R10 E3702 R10 P871\n","U2800 R-1 P1242 R10 E8447 R10 P1847\n","U12 R-1 P2405 R1 E11680 R1 P1289\n"]}]},{"cell_type":"markdown","source":["<a name=\"2.1\"></a>\n","\n","## 2.1 - Tokenized datasets creation\n","\n","---\n","\n","Training PLM or PEARLM requires the learning of a Whitespace tokenizer that possess as vocubary all the entities and relations token. Additionally we need to tokenize the sampled path using this learned tokenizer.\n","\n","To do this we will execute `pathlm/models/lm/tokenize_dataset.py`. This will create our tokenzier that will be stored in `tokenizers/<dataset_name>/WordLevel.json` and our tokenized dataset as hugginface dataset object in `data/<dataset_name>/WordLevel/end-to-end_{sample_size}_{n_hop}_tokenized_dataset.hf[link text](https://)`"],"metadata":{"id":"VTD9h2hx0CqW"}},{"cell_type":"code","source":["! python pathlm/models/lm/tokenize_dataset.py --data {dataset_name} --sample_size {sample_size} --n_hop {n_hop} --nproc {n_proc}"],"metadata":{"id":"r_DlUOIFBPog"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"3\"></a>\n","\n","# 3 - PLM pipeline\n","\n","---\n","\n","*Note: The authors of the paper haven't release the code for this model, our unofficial version is available in our [PEARLM repository]()*\n","\n","PLM [[37]](#p37) (Path Language Model for Explainable Recommendation) is the first attempt of employing causal language models for path reasoning for explainable recommendation.\n","\n","Specifically the model uses a **generic CLM architecture** (e.g. distilgpt2) extended with an additional embedding layer named **type embeddings**. Type embeddings hold the meaning of the `i`-th token and can assume value `0` when the token is a special token like `[BOS]` or `[EOS]`, `1` for an entity or `2` for a relation. This additional embedding layer can support the model in learning the path structures.\n","\n","This model also has an architecture composed by **two heads** one learned for predicting relations and one for entities. These heads are used alternatively during the decoding.\n","\n","![](https://drive.google.com/uc?id=1w_GwOaNPNsITfSTEX-LgkiGiQuqTsE88)\n"],"metadata":{"id":"lDKaZeo3h-52"}},{"cell_type":"markdown","source":["<a name=\"3.1\"></a>\n","## 3.1 Train PLM\n","\n","---\n","\n","The  hyperparameter list is reported as follow:\n","- `--num_epochs`: Max number of epochs.\n","- `--model`: The base huggingface model from where eredit the architecture one from `{distilgpt2, gpt2, gpt2-large}`\n","- `--batch_size`: Batch size.\n","- `--sample_size`: Dataset sample size (to dermine which dataset to use)\n","- `--n_hop`: Dataset hop size (to dermine which dataset to use)\n","- `--logit_processor_type`: Decoding strategy `gcd` PEARLM, empty for PLM\n","- `--n_seq_infer`: Number of sequences generated for each user should be `> k`\n","\n","‚ö†Ô∏è We have already the **precomputed PLM for all datasets**. So you **don't need** to run this command now."],"metadata":{"id":"AcCMTZXJO_C8"}},{"cell_type":"code","source":["! python pathlm/models/lm/plm_main.py --data {dataset_name} --sample_size {sample_size} --n_hop {n_hop} --nproc {n_proc}"],"metadata":{"id":"DsPoIa2Uh-AH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Again like with previous models the train will save during evaluation the topks and topks paths in `results` and the final model in `weights` under the name of `end-to-end@<dataset_name>@plm-rec@<model>@<sample_size>@<n_hop>@<logit_processor_type>`"],"metadata":{"id":"Y5NcF_X198Oh"}},{"cell_type":"markdown","source":["<a name=\"3.2\"></a>\n","## 3.2 Evaluate PLM\n","\n","---\n","\n","Let's now load the paths from `results/`"],"metadata":{"id":"UHGvJcnDPDXP"}},{"cell_type":"code","source":["model_base = 'distilgpt2'\n","logit_constraint = ''\n","curr_model = f'end-to-end@{dataset_name}@plm-rec@{model_base}@{sample_size}@{n_hop}@{logit_constraint}'\n","plm_results_path = f\"results/{dataset_name}/{curr_model}\""],"metadata":{"id":"dBZvD-devXPA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This paths are sorted by path score to produce the final topk of predicted item stored in `results/<dataset>/<curr_model>/top{k}_items.pkl`"],"metadata":{"id":"rInAjRutvXPA"}},{"cell_type":"code","source":["with open(f\"{plm_results_path}/top10_items.pkl\", 'rb') as pred_top_items_file:\n","    plm_item_topks = pickle.load(pred_top_items_file)\n","pred_top_items_file.close()"],"metadata":{"id":"ohqzRw15vXPB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["list(plm_item_topks.items())[:5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711105752901,"user_tz":-60,"elapsed":320,"user":{"displayName":"Giacomo Balloccu","userId":"17057429141399105680"}},"outputId":"63c6e01b-6d47-405d-ea48-ea852df93f52","id":"gSypyuUevXPB"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(15, [1767, 2238, 2386, 1274, 2585, 525, 322, 1841, 566, 2405]),\n"," (49, [1892, 1438, 2435, 2151, 827, 1068, 2785, 1110, 1372, 1744]),\n"," (13, [2011, 2200, 2392, 1274, 960, 525, 599, 2238, 451, 2009]),\n"," (45, [966, 388, 370, 2551, 2016, 591, 2254, 1219, 649, 2399]),\n"," (27, [2102, 867, 1462, 355, 2406, 1841, 11, 2196, 2391, 2235])]"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["And the associated explanations store in `results/<dataset>/<curr_model>/top{k}_paths.pkl`"],"metadata":{"id":"sBYviMRgvXPB"}},{"cell_type":"code","source":["with open(f\"{plm_results_path}/top10_paths.pkl\", 'rb') as pred_top_paths_file:\n","    plm_path_topks = pickle.load(pred_top_paths_file)\n","pred_top_paths_file.close()"],"metadata":{"id":"sUCS3ff6vXPB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["list(plm_path_topks.items())[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711105754927,"user_tz":-60,"elapsed":3,"user":{"displayName":"Giacomo Balloccu","userId":"17057429141399105680"}},"outputId":"7236dc6a-d005-4fcf-f91e-0e7b0755b06e","id":"Z5FtAepPvXPB"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(15,\n"," [['[BOS]', 'U15', 'R-1', 'P1113', 'R5', 'E7174', 'R0', 'P1767'],\n","  ['[BOS]', 'U15', 'R-1', 'P599', 'R9', 'E7629', 'R10', 'P2238'],\n","  ['[BOS]', 'U15', 'R-1', 'P1274', 'R4', 'E12287', 'R7', 'P2386'],\n","  ['[BOS]', 'U15', 'R-1', 'P1274', 'R4', 'E12287', 'R2', 'P1274'],\n","  ['[BOS]', 'U15', 'R-1', 'P599', 'R9', 'E7629', 'R10', 'P2585'],\n","  ['[BOS]', 'U15', 'R-1', 'P2758', 'R9', 'E5831', 'R5', 'P525'],\n","  ['[BOS]', 'U15', 'R-1', 'P2758', 'R7', 'E7248', 'R5', 'P322'],\n","  ['[BOS]', 'U15', 'R-1', 'P1113', 'R2', 'E11914', 'R9', 'P1841'],\n","  ['[BOS]', 'U15', 'R-1', 'P1274', 'R4', 'E12287', 'R2', 'P566'],\n","  ['[BOS]', 'U15', 'R-1', 'P1113', 'R2', 'E11914', 'R9', 'P2405']])"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["command = f'python pathlm/evaluation/evaluate_results.py --dataset {dataset_name} --model plm-rec@{model_base} --k 10'\n","!$command"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711105761939,"user_tz":-60,"elapsed":5414,"user":{"displayName":"Giacomo Balloccu","userId":"17057429141399105680"}},"outputId":"6df04702-f0fd-4f65-f3b0-789db9b8f51f","id":"LT-6QlJzvXPB"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluating rec quality for ['ndcg', 'mrr', 'precision', 'recall', 'serendipity', 'diversity', 'novelty']: 100% 6040/6040 [00:00<00:00, 11434.03it/s]\n","Number of users: 6040, average topk size: 10.00\n","ndcg: 0.27, mrr: 0.21, precision: 0.1, recall: 0.04, serendipity: 0.8, diversity: 0.84, novelty: 0.93, coverage: 0.29\n"]}]},{"cell_type":"markdown","source":["<a name=\"3.3\"></a>\n","## 3.3 Textual Explanation generation\n","\n","---\n","\n","As done before with PGPR and CAFE to convert the explanation path to textual we need to remap the entities and relations to their names. To do so let's import `get_eid_to_name` and `get_rid_to_name` from `pathlm.datasets.data_utils`."],"metadata":{"id":"G1tL-gP8QcpK"}},{"cell_type":"code","source":["from pathlm.datasets.data_utils import get_eid_to_name\n","eid2name = get_eid_to_name(dataset_name)\n","eid2name['0']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"VX1G2D-VwBsj","executionInfo":{"status":"ok","timestamp":1711105763809,"user_tz":-60,"elapsed":1871,"user":{"displayName":"Giacomo Balloccu","userId":"17057429141399105680"}},"outputId":"4a901e47-4148-4cb0-e0f2-d51c783503e8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'The Phantom of the Opera (1925 film)'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["from pathlm.datasets.data_utils import get_rid_to_name\n","rid2name = get_rid_to_name(dataset_name)\n","rid2name['0']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"IuGpC2Nfywd9","executionInfo":{"status":"ok","timestamp":1711105763809,"user_tz":-60,"elapsed":29,"user":{"displayName":"Giacomo Balloccu","userId":"17057429141399105680"}},"outputId":"1b109668-321e-425c-af78-cd982a6874e1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cinematography_by_cinematographer'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["Let's now create the template function to handle the paths in the form `['[BOS]', 'U15', 'R-1', 'P1113', 'R5', 'E7174', 'R0', 'P1767']`"],"metadata":{"id":"gLDRDmoG72zr"}},{"cell_type":"code","source":["def template(path):\n","    if path[0] == \"[BOS]\":\n","        path = path[1:]\n","    for i in range(len(path)):\n","        s = str(path[i])[1:]\n","        if i % 2 == 0: #Entity\n","            path[i] = eid2name[s]\n","        else: #Relation\n","            if s == \"-1\":\n","                path[i] = 'watched'\n","                continue\n","            path[i] = rid2name[s]\n","    u, r, pi, r1, e1, r2, rp  = path\n","    if e1 == 'user':\n","        return f\"You may be interested in {rp} because you {r} {pi} also {r2} by another user\"\n","    else:\n","        return f\"You may be interested in {rp} because you {r} {pi} also {r2} by {e1}\""],"metadata":{"id":"sz0wTvKPwH1G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's convert the explanation paths for a random user to textual explanation"],"metadata":{"id":"Kw-U8gq98P1O"}},{"cell_type":"code","source":["import collections\n","plm_textual_exps = collections.defaultdict(list)\n","random_user = random.randint(0, len(plm_path_topks.keys()))\n","for i, pid_exp_tuple in enumerate(plm_path_topks[random_user]):\n","    exp = pid_exp_tuple\n","    pid = pid_exp_tuple[-1][-1]\n","    plm_textual_exps[random_user].append([pid, template(exp)])"],"metadata":{"id":"gBZFrcxxQgY0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plm_textual_exps[random_user]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dnhOT2TGwq6Q","executionInfo":{"status":"ok","timestamp":1711105763810,"user_tz":-60,"elapsed":28,"user":{"displayName":"Giacomo Balloccu","userId":"17057429141399105680"}},"outputId":"f9f77f5c-9440-4a80-cac8-2a611e02d29a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['1',\n","  'You may be interested in Gladiator (2000 film) because you watched The Perfect Storm (film) also produced_by_prodcompany by United States'],\n"," ['5',\n","  'You may be interested in Erin Brockovich (film) because you watched X-Men (film) also composed_by_composer by Newton Thomas Sigel'],\n"," ['7',\n","  'You may be interested in Shanghai Noon because you watched The Patriot (2000 film) also belong_to_category by David Brenner (editor)'],\n"," ['8',\n","  'You may be interested in Magnolia (film) because you watched The Patriot (2000 film) also produced_by_producer by David Brenner (editor)'],\n"," ['0',\n","  'You may be interested in U-571 (film) because you watched The Perfect Storm (film) also related_to_wikipage by Phillip Noyce'],\n"," ['0',\n","  'You may be interested in Frequency (film) because you watched The Patriot (2000 film) also produced_by_producer by David Brenner (editor)'],\n"," ['9',\n","  'You may be interested in Braveheart (1925 film) because you watched The Perfect Storm (film) also watched by Phillip Noyce'],\n"," ['9',\n","  'You may be interested in Shaft (2000 film) because you watched Gladiator (2000 film) also starred_by_actor by John Madden (director)'],\n"," ['6',\n","  'You may be interested in Mission: Impossible II because you watched The Perfect Storm (film) also composed_by_composer by Phillip Noyce'],\n"," ['9',\n","  'You may be interested in Mission: Impossible (film) because you watched X-Men (film) also composed_by_composer by Newton Thomas Sigel']]"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["As you may see many of these explanations contain **hallucinations**. Hallucinations can arise within an explanation when a model incorrectly establishes **incoherent semantic relations** between entities in the KG, e.g., when user-item connections extend beyond mere interaction relations, which would constitute the sole viable option in the KG.\n","\n","Incorrect semantics may lead to provide explanations connecting the two by a \"starred by\" relation, which is coherent only between an actor and a movie item as for the KG. Incoherence can also manifest between entities that are semantically linked in the real world but do not have such corresponding relationships in the KG (e.g., entity \"Johnny Depp\", linked by the relation \"starred in\", to the item \"interstellar\", which does not exist in the underlying KG).\n","\n","Such inaccuracies in explanations compromise the fundamental rationale for utilizing a KG, as the factual truths presented in the explanations become misaligned and incoherent with the underlying KG."],"metadata":{"id":"TQTYLrtj8Y_r"}},{"cell_type":"markdown","source":["To measure the extend of these phenomena let's calculate what is the rate of corrupted paths among the predicted ones. To do so let's use `` function from `` module"],"metadata":{"id":"YDE-s9WXAQqM"}},{"cell_type":"code","source":["command = f'python pathlm/models/lm/assess_faithfulness.py --dataset {dataset_name} --model plm-rec@{model_base} --k 10'\n","!$command"],"metadata":{"id":"Uc6DGi88AlXT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711106718224,"user_tz":-60,"elapsed":57942,"user":{"displayName":"Giacomo Balloccu","userId":"17057429141399105680"}},"outputId":"793de68f-9d74-4a66-c0ea-60fd779cfaae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Inputing a PLM\n","Loading KG\n","Load user of size 6040\n","Load product of size 2983\n","Load cinematographer of size 236\n","Load prodcompany of size 304\n","Load composer of size 292\n","Load category of size 1821\n","Load actor of size 2330\n","Load country of size 32\n","Load editor of size 198\n","Load producer of size 458\n","Load writter of size 289\n","Load director of size 117\n","Load wikipage of size 4700\n","Load cinematography_by_cinematographer of size 1481\n","Load produced_by_prodcompany of size 4330\n","Load composed_by_composer of size 1999\n","Load belong_to_category of size 40499\n","Load starred_by_actor of size 9411\n","Load produced_in_country of size 311\n","Load edited_by_editor of size 1153\n","Load produced_by_producer of size 1602\n","Load wrote_by_writter of size 845\n","Load directed_by_director of size 367\n","Load related_to_wikipage of size 143523\n","Invalid users: 0, invalid items: 0\n","Load review of size 556989\n","Loading from  data/ml1m/preprocessed  the dataset  ml1m\n","(205879, 3)\n","(193338, 3)\n","{0: 'cinematography_by_cinematographer', 1: 'produced_by_prodcompany', 2: 'composed_by_composer', 3: 'belong_to_category', 4: 'starred_by_actor', 5: 'produced_in_country', 6: 'edited_by_editor', 7: 'produced_by_producer', 8: 'wrote_by_writter', 9: 'directed_by_director', 10: 'related_to_wikipage', -1: 'watched'}\n","Creating augmented kg\n","Created augmented kg\n","Creating token index\n","Created token index\n","Check faithfulness\n","2024-03-22 11:25:05.114862: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-03-22 11:25:05.115069: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-03-22 11:25:05.249019: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-03-22 11:25:07.118091: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Average rate of valid sequences per user: 0.0016059602649006624\n","60299/60400 corrupted sequences, specifically at position Counter({5: 22325, 2: 20588, 3: 11795, 4: 3399, 6: 2192})\n","Examples of corrupted sequence for user 6039:\n","Raw corrupted sequence:\n","['U6039', 'R-1', 'P2102', 'R0', 'E3984', 'R0', 'P1329'], incoherence for P1329\n","Plain text corrupted sequence:\n","['U6039', 'interacted', 'The Godfather Part II', 'cinematography_by_cinematographer', 'Gordon Willis', 'cinematography_by_cinematographer', 'Back to the Future'], incoherence for Back to the Future\n","\n","\n","Raw corrupted sequence:\n","['U6039', 'R-1', 'P2102', 'R0', 'E3984', 'R10', 'P538'], incoherence for R10\n","Plain text corrupted sequence:\n","['U6039', 'interacted', 'The Godfather Part II', 'cinematography_by_cinematographer', 'Gordon Willis', 'related_to_wikipage', 'American Graffiti'], incoherence for related_to_wikipage\n","\n","\n","Raw corrupted sequence:\n","['U6039', 'R-1', 'P1500', 'R2', 'E9088', 'R7', 'P152'], incoherence for R7\n","Plain text corrupted sequence:\n","['U6039', 'interacted', 'Chinatown (1974 film)', 'composed_by_composer', 'Jerry Goldsmith', 'produced_by_producer', 'Lethal Weapon'], incoherence for produced_by_producer\n","\n","\n","Raw corrupted sequence:\n","['U6039', 'R-1', 'P2102', 'R0', 'E3984', 'R0', 'P1762'], incoherence for P1762\n","Plain text corrupted sequence:\n","['U6039', 'interacted', 'The Godfather Part II', 'cinematography_by_cinematographer', 'Gordon Willis', 'cinematography_by_cinematographer', 'Fast Times at Ridgemont High'], incoherence for Fast Times at Ridgemont High\n","\n","\n","Raw corrupted sequence:\n","['U6039', 'R-1', 'P1359', 'R4', 'E11101', 'R9', 'P1188'], incoherence for R4\n","Plain text corrupted sequence:\n","['U6039', 'interacted', 'Star Wars (film)', 'starred_by_actor', 'Gilbert Taylor', 'directed_by_director', 'The Princess Bride (film)'], incoherence for starred_by_actor\n","\n","\n","Raw corrupted sequence:\n","['U6039', 'R-1', 'P1359', 'R4', 'E11101', 'R8', 'P2144'], incoherence for R4\n","Plain text corrupted sequence:\n","['U6039', 'interacted', 'Star Wars (film)', 'starred_by_actor', 'Gilbert Taylor', 'wrote_by_writter', 'Strangers on a Train (film)'], incoherence for starred_by_actor\n","\n","\n","Raw corrupted sequence:\n","['U6039', 'R-1', 'P1500', 'R2', 'E9088', 'R6', 'P2102'], incoherence for R6\n","Plain text corrupted sequence:\n","['U6039', 'interacted', 'Chinatown (1974 film)', 'composed_by_composer', 'Jerry Goldsmith', 'edited_by_editor', 'The Godfather Part II'], incoherence for edited_by_editor\n","\n","\n","Raw corrupted sequence:\n","['U6039', 'R-1', 'P2295', 'R9', 'E7043', 'R1', 'P79'], incoherence for R9\n","Plain text corrupted sequence:\n","['U6039', 'interacted', 'Young Frankenstein', 'directed_by_director', 'John Frankenheimer', 'produced_by_prodcompany', 'Full Metal Jacket'], incoherence for directed_by_director\n","\n","\n","Raw corrupted sequence:\n","['U6039', 'R-1', 'P2102', 'R0', 'E3984', 'R2', 'P2484'], incoherence for R2\n","Plain text corrupted sequence:\n","['U6039', 'interacted', 'The Godfather Part II', 'cinematography_by_cinematographer', 'Gordon Willis', 'composed_by_composer', 'Jaws (film)'], incoherence for composed_by_composer\n","\n","\n","Raw corrupted sequence:\n","['U6039', 'R-1', 'P1500', 'R2', 'E9088', 'R6', 'P2859'], incoherence for R6\n","Plain text corrupted sequence:\n","['U6039', 'interacted', 'Chinatown (1974 film)', 'composed_by_composer', 'Jerry Goldsmith', 'edited_by_editor', 'Dog Day Afternoon'], incoherence for edited_by_editor\n","\n","\n"]}]},{"cell_type":"markdown","source":["<a name=\"4\"></a>\n","\n","# 4. PEARLM pipeline\n","---\n","\n","   "],"metadata":{"id":"K2A2RjtyjFi1"}},{"cell_type":"markdown","source":["PEARLM [[38]](#p38) (Path-Explainable Accurate Language Model for Explainable Recommendation) is another causual language model for path reasoning and aims to resolve the \"hallucination\" issue of PLM, its dependency from pretrained KGE embedding and PLM inference scalability issues. Specifically the main difference are:\n","\n","- **Single Head for Prediction**: Unlike PLM, which uses multiple heads to handle entities and relations, assuming their independence, PEARLM employs a single head. This design choice simplifies the model's complexity and allows for the incorporation of all previous context into the model's learning process.\n","- **Direct Embedding Learning**: Unlike its predecessors that rely on pretrained KGEs for initializing embeddings, PEARLM learns embeddings directly from the knowledge graph paths. This method not only fully leverages the model's capacity but also supports the concept that sequential learning from paths effectively captures the intricate relationships within the knowledge graph. Furthermore, PEARLM's design draws parallels between Graph Neural Networks (GNNs) and Causal Language Models (CLMs), likening GNNs' breadth-first search (BFS) for capturing immediate neighborhood information to the depth-first search (DFS)-like path generation of CLMs. This analogy underscores PEARLM's ability to delve deeper into the knowledge graph, unearthing distant but relevant relationships essential for generating explainable recommendations.\n","- **Graph Constraint Decoding**: To ensure the fidelity of generated paths to the underlying knowledge graph, PEARLM introduces \"Graph Constraint Decoding\" (GCD). This feature guarantees that at the decoding phase, the model adheres to the structure of the knowledge graph, thereby addressing the issue of hallucinated or factually incorrect paths generated by other models.\n","\n","![](https://drive.google.com/uc?id=1leZB7KU-KgtnZPga2j8EXimktkNFAk1K)\n"],"metadata":{"id":"Ngq6jFsYpiPM"}},{"cell_type":"markdown","source":["<a name=\"4.1\"></a>\n","## 4.1 Train PEARLM\n","\n","---\n","\n","The  hyperparameter list is reported as follow:\n","- `--num_epochs`: Max number of epochs.\n","- `--model`: The base huggingface model from where eredit the architecture one from `{distilgpt2, gpt2, gpt2-large}`\n","- `--batch_size`: Batch size.\n","- `--sample_size`: Dataset sample size (to dermine which dataset to use)\n","- `--n_hop`: Dataset hop size (to dermine which dataset to use)\n","- `--logit_processor_type`: Decoding strategy `gcd PEARLM, empty for PLM\n","- `--n_seq_infer`: Number of sequences generated for each user should be `> k`\n","\n","‚ö†Ô∏è We have already the **precomputed PEARLM for all datasets**. So you **don't need** to run this command now. The hugginface model will be stored into `end-to-end@<dataset_name>@<model>@<sample_size>@<n_hop>@<logit_processor_type>`.\n","\n","‚è≤Ô∏è"],"metadata":{"id":"2eqrtV6cPPCJ"}},{"cell_type":"code","source":["! python pathlm/models/lm/pearlm_main.py --data {dataset_name} --sample_size {sample_size} --n_hop {n_hop} --nproc {n_proc}"],"metadata":{"id":"C8qroFIWjJ3F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"4.2\"></a>\n","## 4.2 Evaluate PEARLM\n","\n","---"],"metadata":{"id":"6hSuv8o9QYES"}},{"cell_type":"code","source":["model_base = 'distilgpt2'\n","logit_constraint = 'gcd'\n","curr_model = f'end-to-end@{dataset_name}@{model_base}@{sample_size}@{n_hop}@{logit_constraint}'\n","pearlm_results_path = f\"results/{dataset_name}/{curr_model}\""],"metadata":{"id":"yrZ1F9nX8ava"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This paths are sorted by path score to produce the final topk of predicted item stored in `results/<dataset>/<curr_model>/top{k}_items.pkl`"],"metadata":{"id":"2jpVcfsa8avb"}},{"cell_type":"code","source":["with open(f\"{pearlm_results_path}/top10_items.pkl\", 'rb') as pred_top_items_file:\n","    pearlm_item_topks = pickle.load(pred_top_items_file)\n","pred_top_items_file.close()"],"metadata":{"id":"pavchHvL8avb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["list(pearlm_item_topks.items())[:5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7_cZejPqJzAz","executionInfo":{"status":"ok","timestamp":1711106727908,"user_tz":-60,"elapsed":3,"user":{"displayName":"Giacomo Balloccu","userId":"17057429141399105680"}},"outputId":"51114114-1fe9-4fa1-ff47-4493bde67f79"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(1, [934, 1963, 1313, 1123, 717, 2196, 531, 1457, 763, 1775]),\n"," (2, [1289, 455, 1372, 2290, 484, 319, 2812, 2151, 316, 272]),\n"," (4, [2708, 2605, 2037, 1554, 725, 1635, 2673, 2712, 1011, 2240]),\n"," (7, [2345, 501, 2960, 1411, 60, 44, 2341, 308, 330, 379]),\n"," (9, [601, 403, 2662, 1613, 118, 466, 2345, 2341, 1113, 1020])]"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["And the associated explanations store in `results/<dataset>/<curr_model>/top{k}_paths.pkl`"],"metadata":{"id":"uA3x8_7W8avb"}},{"cell_type":"code","source":["with open(f\"{pearlm_results_path}/top10_paths.pkl\", 'rb') as pred_top_paths_file:\n","    pearlm_path_topks = pickle.load(pred_top_paths_file)\n","pred_top_paths_file.close()"],"metadata":{"id":"shyM7LLO8avb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["list(pearlm_path_topks.items())[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3dagt9niKJ9f","executionInfo":{"status":"ok","timestamp":1711106732186,"user_tz":-60,"elapsed":383,"user":{"displayName":"Giacomo Balloccu","userId":"17057429141399105680"}},"outputId":"b48d8b8b-95f0-4e06-a559-4296e6b37364"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1,\n"," [['[BOS]', 'U1', 'R-1', 'P2555', 'R3', 'E7934', 'R3', 'P934'],\n","  ['[BOS]', 'U1', 'R-1', 'P2444', 'R7', 'E8039', 'R7', 'P1963'],\n","  ['[BOS]', 'U1', 'R-1', 'P2972', 'R1', 'E8946', 'R1', 'P1313'],\n","  ['[BOS]', 'U1', 'R-1', 'P2972', 'R1', 'E8946', 'R1', 'P1123'],\n","  ['[BOS]', 'U1', 'R-1', 'P2555', 'R3', 'E7934', 'R3', 'P717'],\n","  ['[BOS]', 'U1', 'R-1', 'P1249', 'R3', 'E7934', 'R3', 'P2196'],\n","  ['[BOS]', 'U1', 'R-1', 'P2151', 'R1', 'E11970', 'R1', 'P531'],\n","  ['[BOS]', 'U1', 'R-1', 'P2972', 'R1', 'E8946', 'R1', 'P1457'],\n","  ['[BOS]', 'U1', 'R-1', 'P1249', 'R3', 'E7934', 'R3', 'P763'],\n","  ['[BOS]', 'U1', 'R-1', 'P1249', 'R3', 'E7934', 'R3', 'P1775']])"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["command = f'python pathlm/evaluation/evaluate_results.py --dataset {dataset_name} --model {curr_model} --k 10'\n","!$command"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R2XMx9oX0zcB","executionInfo":{"status":"ok","timestamp":1711106739633,"user_tz":-60,"elapsed":5585,"user":{"displayName":"Giacomo Balloccu","userId":"17057429141399105680"}},"outputId":"1413b570-9202-41aa-9e21-3de18bcb50f7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluating rec quality for ['ndcg', 'mrr', 'precision', 'recall', 'serendipity', 'diversity', 'novelty']: 100% 6040/6040 [00:00<00:00, 10770.22it/s]\n","Number of users: 6040, average topk size: 10.00\n","ndcg: 0.37, mrr: 0.28, precision: 0.12, recall: 0.07, serendipity: 0.94, diversity: 0.84, novelty: 0.93, coverage: 0.8\n"]}]},{"cell_type":"markdown","source":["<a name=\"4.3\"></a>\n","## 4.3 Textual explanation generation\n","\n","---\n","\n","Let's convert some PEARLM predicted explanation paths to textual explanations. To do so we will leverage the previously defined `template` function and the dictionaries `eid2name` and `rid2name`."],"metadata":{"id":"tSC__ik11FHI"}},{"cell_type":"code","source":["import collections\n","pearlm_textual_exps = collections.defaultdict(list)\n","random_user = random.randint(0, len(pearlm_path_topks.keys()))\n","for i, pid_exp_tuple in enumerate(pearlm_path_topks[random_user]):\n","    exp = pid_exp_tuple\n","    pid = pid_exp_tuple[-1][-1]\n","    pearlm_textual_exps[random_user].append([pid, template(exp)])"],"metadata":{"id":"O-FEhLma6dLo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pearlm_textual_exps[random_user]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711106854122,"user_tz":-60,"elapsed":2,"user":{"displayName":"Giacomo Balloccu","userId":"17057429141399105680"}},"outputId":"1139e941-88ab-4c19-bdcc-175d555d15f7","id":"fBZLrWa61KL_"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['9',\n","  'You may be interested in Breathless (1983 film) because you watched RoboCop also produced_by_prodcompany by Orion Pictures'],\n"," ['6',\n","  'You may be interested in Mission: Impossible II because you watched The Rock (film) also composed_by_composer by Hans Zimmer'],\n"," ['9',\n","  'You may be interested in Time Bandits because you watched Star Wars Episode I: The Phantom Menace also related_to_wikipage by 1999 in film'],\n"," ['9',\n","  'You may be interested in Mission to Mars because you watched Predator (film) also wrote_by_writter by John Thomas (screenwriter)'],\n"," ['4',\n","  'You may be interested in The Man with the Golden Gun (film) because you watched Thunderball (film) also produced_by_prodcompany by Eon Productions'],\n"," ['0',\n","  'You may be interested in The Mummy (1999 film) because you watched Star Wars Episode I: The Phantom Menace also related_to_wikipage by 1999 in film'],\n"," ['0',\n","  'You may be interested in Speed 2: Cruise Control because you watched Godfather (1991 film) also related_to_wikipage by Soundtrack'],\n"," ['0',\n","  'You may be interested in The Emerald Forest because you watched Star Wars Episode I: The Phantom Menace also related_to_wikipage by Avatar (2009 film)'],\n"," ['2',\n","  'You may be interested in Dune (film) because you watched Godfather (1991 film) also related_to_wikipage by Ensemble cast'],\n"," ['8',\n","  \"You may be interested in Logan's Run (film) because you watched Thunderball (film) also produced_by_prodcompany by United Artists\"]]"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["command = f'python pathlm/models/lm/assess_faithfulness.py --dataset {dataset_name} --model {model_base} --k 10'\n","!$command"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HtLzmzFb6sjg","executionInfo":{"status":"ok","timestamp":1711106923060,"user_tz":-60,"elapsed":44010,"user":{"displayName":"Giacomo Balloccu","userId":"17057429141399105680"}},"outputId":"06214f97-d08b-4a2d-f001-f4ed2bcc3956"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading KG\n","Load user of size 6040\n","Load product of size 2983\n","Load cinematographer of size 236\n","Load prodcompany of size 304\n","Load composer of size 292\n","Load category of size 1821\n","Load actor of size 2330\n","Load country of size 32\n","Load editor of size 198\n","Load producer of size 458\n","Load writter of size 289\n","Load director of size 117\n","Load wikipage of size 4700\n","Load cinematography_by_cinematographer of size 1481\n","Load produced_by_prodcompany of size 4330\n","Load composed_by_composer of size 1999\n","Load belong_to_category of size 40499\n","Load starred_by_actor of size 9411\n","Load produced_in_country of size 311\n","Load edited_by_editor of size 1153\n","Load produced_by_producer of size 1602\n","Load wrote_by_writter of size 845\n","Load directed_by_director of size 367\n","Load related_to_wikipage of size 143523\n","Invalid users: 0, invalid items: 0\n","Load review of size 556989\n","Loading from  data/ml1m/preprocessed  the dataset  ml1m\n","(205879, 3)\n","(193338, 3)\n","{0: 'cinematography_by_cinematographer', 1: 'produced_by_prodcompany', 2: 'composed_by_composer', 3: 'belong_to_category', 4: 'starred_by_actor', 5: 'produced_in_country', 6: 'edited_by_editor', 7: 'produced_by_producer', 8: 'wrote_by_writter', 9: 'directed_by_director', 10: 'related_to_wikipage', -1: 'watched'}\n","Creating augmented kg\n","Created augmented kg\n","Creating token index\n","Created token index\n","Check faithfulness\n","2024-03-22 11:28:31.628737: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-03-22 11:28:31.628818: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-03-22 11:28:31.631448: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-03-22 11:28:32.714595: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Average rate of valid sequences per user: 0.999950331125828\n","3/60400 corrupted sequences, specifically at position Counter({6: 3})\n","Examples of corrupted sequence for user 6029:\n"]}]},{"cell_type":"markdown","source":["# References"],"metadata":{"id":"YitklJg1bqj-"}},{"cell_type":"markdown","metadata":{"id":"tsT2uJvMY9GA"},"source":["<a name=\"p1\">[1]</a> F. Maxwell Harper, Joseph A. Konstan:\n","The MovieLens Datasets: History and Context. ACM Trans. Interact. Intell. Syst. 5(4): 19:1-19:19 (2016)\n","\n","<a name=\"p2\">[2]</a> Markus Schedl: The LFM-1b Dataset for Music Retrieval and Recommendation. ICMR 2016: 103-110\n","\n","<a name=\"p3\">[3]</a> S√∂ren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, Zachary G. Ives:\n","DBpedia: A Nucleus for a Web of Open Data. ISWC/ASWC 2007: 722-735\n","\n","<a name=\"p4\">[4]</a> Denny Vrandecic, Markus Kr√∂tzsch:\n","Wikidata: a free collaborative knowledgebase. Commun. ACM 57(10): 78-85 (2014)\n","\n","<a name=\"p5\">[5]</a> Yixin Cao, Xiang Wang, Xiangnan He, Zikun Hu, Tat-Seng Chua:\n","Unifying Knowledge Graph Learning and Recommendation: Towards a Better Understanding of User Preferences. WWW 2019: 151-161\n","\n","\n","<a name=\"p6\">[6]</a> Qingyao Ai, Vahid Azizi, Xu Chen, Yongfeng Zhang:\n","Learning Heterogeneous Knowledge Base Embeddings for Explainable Recommendation. Algorithms 11(9): 137 (2018)\n","\n","<a name=\"p7\">[7]</a> Kurt D. Bollacker, Colin Evans, Praveen K. Paritosh, Tim Sturge, Jamie Taylor:\n","Freebase: a collaboratively created graph database for structuring human knowledge. SIGMOD Conference 2008: 1247-1250\n","\n","<a name=\"p8\">[8]</a> Wayne Xin Zhao, Gaole He, Kunlin Yang, Hongjian Dou, Jin Huang, Siqi Ouyang, Ji-Rong Wen:\n","KB4Rec: A Data Set for Linking Knowledge Bases with Recommender Systems. Data Intell. 1(2): 121-136 (2019)\n","\n","<a name=\"p9\">[9]</a> Yongfeng Zhang, Qingyao Ai, Xu Chen, W. Bruce Croft:\n","Joint Representation Learning for Top-N Recommendation with Heterogeneous Information Sources. CIKM 2017: 1449-1458\n","\n","<a name=\"p10\">[10]</a> Yikun Xian, Zuohui Fu, S. Muthukrishnan, Gerard de Melo, Yongfeng Zhang:\n","Reinforcement Knowledge Graph Reasoning for Explainable Recommendation. SIGIR 2019: 285-294\n","\n","<a name=\"p11\">[11]</a> Antoine Bordes, Nicolas Usunier, Alberto Garc√≠a-Dur√°n, Jason Weston, Oksana Yakhnenko:\n","Translating Embeddings for Modeling Multi-relational Data. NIPS 2013: 2787-2795\n","\n","<a name=\"p12\">[12]</a> Yikun Xian, Zuohui Fu, Handong Zhao, Yingqiang Ge, Xu Chen, Qiaoying Huang, Shijie Geng, Zhou Qin, Gerard de Melo, S. Muthukrishnan, Yongfeng Zhang:\n","CAFE: Coarse-to-Fine Neural Symbolic Reasoning for Explainable Recommendation. CIKM 2020: 1645-1654\n","\n","<a name=\"p13\">[13]</a> Zhu Sun, Jie Yang, Jie Zhang, Alessandro Bozzon, Long-Kai Huang, Chi Xu:\n","Recurrent knowledge graph embedding for effective recommendation. RecSys 2018: 297-305\n","\n","<a name=\"p14\">[14]</a> Hongwei Wang, Fuzheng Zhang, Miao Zhao, Wenjie Li, Xing Xie, Minyi Guo:\n","Multi-Task Feature Learning for Knowledge Graph Enhanced Recommendation. CoRR abs/1901.08907 (2019)\n","\n","<a name=\"p15\">[15]</a> Xiang Wang, Tinglin Huang, Dingxian Wang, Yancheng Yuan, Zhenguang Liu, Xiangnan He, Tat-Seng Chua:\n","Learning Intents behind Interactions with Knowledge Graph for Recommendation. WWW 2021: 878-887\n","\n","<a name=\"p16\">[16]</a> Song, Weiping, Zhijian Duan, Ziqing Yang, Hao Zhu, Ming Zhang, and Jian Tang. \"Explainable knowledge graph-based recommendation via deep reinforcement learning.\" arXiv preprint arXiv:1906.09506 (2019).\n","\n","<a name=\"p17\">[17]</a>\tHongwei Wang, Fuzheng Zhang, Jialin Wang, Miao Zhao, Wenjie Li, Xing Xie, Minyi Guo:\n","RippleNet: Propagating User Preferences on the Knowledge Graph for Recommender Systems. CIKM 2018: 417-426\n","\n","<a name=\"p18\">[18]</a> Xiang Wang, Dingxian Wang, Canran Xu, Xiangnan He, Yixin Cao, Tat-Seng Chua:\n","Explainable Reasoning over Knowledge Graphs for Recommendation. AAAI 2019: 5329-5336\n","\n","<a name=\"p19\">[19]</a> Binbin Hu, Chuan Shi, Wayne Xin Zhao, Philip S. Yu: Leveraging Meta-path based Context for Top- N Recommendation with A Neural Co-Attention Model. KDD 2018: 1531-1540\n","\n","<a name=\"p20\">[20]</a>\n","Chuan Shi, Binbin Hu, Wayne Xin Zhao, Philip S. Yu:\n","Heterogeneous Information Network Embedding for Recommendation. CoRR abs/1711.10730 (2017)\n","\n","<a name=\"p21\">[21]</a> Xiaowen Huang, Quan Fang, Shengsheng Qian, Jitao Sang, Yan Li, Changsheng Xu:\n","Explainable Interaction-driven User Modeling over Knowledge Graph for Sequential Recommendation. ACM Multimedia 2019: 548-556\n","\n","<a name=\"p22\">[22]</a> Song, Weiping, et al. \"Explainable knowledge graph-based recommendation via deep reinforcement learning.\" arXiv preprint arXiv:1906.09506 (2019).\n","\n","<a name=\"p23\">[23]</a> Chang-You Tai, Liang-Ying Huang, Chien-Kun Huang, Lun-Wei Ku:\n","User-Centric Path Reasoning towards Explainable Recommendation. SIGIR 2021: 879-889\n","\n","<a name=\"p24\">[24]</a> Xiting Wang, Kunpeng Liu, Dongjie Wang, Le Wu, Yanjie Fu, Xing Xie:\n","Multi-level Recommendation Reasoning over Knowledge Graphs with Reinforcement Learning. WWW 2022: 2098-2108\n","\n","<a name=\"p25\">[25]</a> Danyang Liu, Jianxun Lian, Zheng Liu, Xiting Wang, Guangzhong Sun, Xing Xie:\n","Reinforced Anchor Knowledge Graph Generation for News Recommendation Reasoning. KDD 2021: 1055-1065\n","\n","<a name=\"p26\">[26]</a> Zhen Wang, Jianwen Zhang, Jianlin Feng, Zheng Chen:\n","Knowledge Graph Embedding by Translating on Hyperplanes. AAAI 2014: 1112-\n","\n","<a name=\"p27\">[27]</a> Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, Jian Tang:\n","RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space. ICLR (Poster) 2019\n","\n","<a name=\"p28\">[28]</a>  Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, Xuan Zhu:\n","Learning Entity and Relation Embeddings for Knowledge Graph Completion. AAAI 2015: 2181-2187\n","\n","<a name=\"p29\">[29]</a>  Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel:\n","Convolutional 2D Knowledge Graph Embeddings. AAAI 2018: 1811-1818\n","\n","<a name=\"p30\">[30]</a> Ni Lao, Tom M. Mitchell, William W. Cohen:\n","Random Walk Inference and Learning in A Large Scale Knowledge Base. EMNLP 2011: 529-539\n","\n","\n","<a name=\"p31\">[31]</a> Yining Wang, Liwei Wang, Yuanzhi Li, Di He, Tie-Yan Liu:\n","A Theoretical Analysis of NDCG Type Ranking Measures. COLT 2013: 25-54\n","\n","<a name=\"p32\">[32]</a> Giacomo Balloccu, Ludovico Boratto, Gianni Fenu, and Mirko Marras. 2022. Post Processing Recommender Systems with Knowledge Graphs for Recency, Popularity, and Diversity of Explanations. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '22). Association for Computing Machinery, New York, NY, USA, 646‚Äì656. https://doi.org/10.1145/3477495.3532041\n","\n","<a name=\"p33\">[33]</a> Balloccu G, Boratto L, Fenu G, Marras M. Reinforcement recommendation reasoning through knowledge graphs for explanation path quality. Knowledge-Based Systems. 2023 Jan 25;260:110098.\n","\n","<a name=\"p34\">[34]</a> Dess√¨ D, Fenu G, Marras M, Reforgiato Recupero D. Coco: Semantic-enriched collection of online courses at scale with experimental use cases. InTrends and Advances in Information Systems and Technologies: Volume 2 6 2018 (pp. 1386-1396). Springer International Publishing.\n","\n","<a name=\"p35\">[35]</a>  Ni J, Li J, McAuley J. Justifying recommendations using distantly-labeled reviews and fine-grained aspects. InProceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP) 2019 Nov (pp. 188-197).\n","\n","<a name=\"p36\">[36]</a> Balloccu G, Boratto L, Cancedda C, Fenu G, Marras M. Knowledge is power, understanding is impact: Utility and beyond goals, explanation quality, and fairness in path reasoning recommendation. InEuropean Conference on Information Retrieval 2023 Mar 16 (pp. 3-19). Cham: Springer Nature Switzerland.\n","\n","<a name=\"p37\">[37]</a> Geng S, Fu Z, Tan J, Ge Y, De Melo G, Zhang Y. Path language modeling over knowledge graphsfor explainable recommendation. InProceedings of the ACM Web Conference 2022 2022 Apr 25 (pp. 946-955).\n","\n","<a name=\"p38\">[38]</a> Balloccu G, Boratto L, Cancedda C, Fenu G, Marras M. Faithful Path Language Modelling for Explainable Recommendation over Knowledge Graph. arXiv preprint arXiv:2310.16452. 2023 Oct 25.\n","\n","<a name=\"p39\">[39]</a> Afreen N, Balloccu G, Boratto L, Fenu G, Marras M. Towards explainable educational recommendation through path reasoning methods. InCEUR WORKSHOP PROCEEDINGS 2023 (Vol. 3448, pp. 131-136). CEUR-WS."]}]}